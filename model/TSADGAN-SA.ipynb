{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TSADGAN-SA_github用.ipynb","provenance":[],"collapsed_sections":["LqZqmp8-boMe","uZVqF6nhznXf"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jVhHscIMEtMz"},"source":["# TSADGAN-SA for Tensorflow 2.0 "]},{"cell_type":"markdown","source":["##Prepare1"],"metadata":{"id":"LqZqmp8-boMe"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YGCVxWVfcydY","executionInfo":{"status":"ok","timestamp":1645666332106,"user_tz":-540,"elapsed":20509,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"outputId":"9ff79500-f803-4870-f852-5a9c5bb21877"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/salesforce/Merlion.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abkm-v9qQzOZ","executionInfo":{"status":"ok","timestamp":1645666344258,"user_tz":-540,"elapsed":12160,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"outputId":"16d3eadb-845c-4bca-8d30-116c366bc140"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Merlion'...\n","remote: Enumerating objects: 2851, done.\u001b[K\n","remote: Counting objects: 100% (2711/2711), done.\u001b[K\n","remote: Compressing objects: 100% (1158/1158), done.\u001b[K\n","remote: Total 2851 (delta 2105), reused 2014 (delta 1553), pack-reused 140\u001b[K\n","Receiving objects: 100% (2851/2851), 87.19 MiB | 13.06 MiB/s, done.\n","Resolving deltas: 100% (2114/2114), done.\n","Checking out files: 100% (234/234), done.\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append(\"/content/Merlion/ts_datasets\")\n","sys.path.append(\"/content/Merlion/ts_datasets/ts_datasets\")\n","sys.path.append('/content/drive/My Drive/LAC/Q2/implement/model/')"],"metadata":{"id":"q5E94ErEceEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from anomaly.nab import NAB\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import f1_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from utils import plot, plot_ts, plot_rws, plot_error, unroll_ts"],"metadata":{"id":"2G1cStiHTVCG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Prepare2"],"metadata":{"id":"P2-ysA-_jy44"}},{"cell_type":"code","source":["dataset_idx = 0\n","datasets = NAB(subset=\"realAWSCloudwatch\")\n","time_series, metadata = datasets[dataset_idx]\n","X_train, y_train = time_series[metadata[\"trainval\"]], metadata[metadata[\"trainval\"]==True][\"anomaly\"]\n","X_test, y_test = time_series[~metadata[\"trainval\"]], metadata[metadata[\"trainval\"]==False][\"anomaly\"]\n","sum(y_train), sum(y_test)"],"metadata":{"id":"hu22v7LzRBG1","executionInfo":{"status":"ok","timestamp":1645666538438,"user_tz":-540,"elapsed":3077,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6cb72dd8-e41c-4bb1-fecc-133716334a52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Time series /content/Merlion/data/nab/realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv (index 8) has timestamp duplicates. Kept first values.\n","Time series /content/Merlion/data/nab/realAWSCloudwatch/ec2_network_in_5abac7.csv (index 11) has timestamp duplicates. Kept first values.\n"]},{"output_type":"execute_result","data":{"text/plain":["(0, 402)"]},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","source":["##Prepare3"],"metadata":{"id":"7jAUwIoWjpGB"}},{"cell_type":"code","source":["X_train, y_train = X_train[y_train!=True], y_train[y_train!=True]"],"metadata":{"id":"agACWTi9a_cs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sum(y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HCwslI-PbZhq","executionInfo":{"status":"ok","timestamp":1645666538439,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"outputId":"a56196e6-62b7-442f-da12-6ca4d69587c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"49rVid5gYqAI"},"source":["#complement missing value\n","imp = SimpleImputer()\n","X_train = imp.fit_transform(X_train)\n","X_test = imp.fit_transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(X_train), len(y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ibbKifjXUsXv","executionInfo":{"status":"ok","timestamp":1645666538441,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"outputId":"8e7e0fab-0bc3-46d9-8d8c-d019ba1de4dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(605, 605)"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["(np.nan in X_train), (np.nan in X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG_N9AxaUxqW","executionInfo":{"status":"ok","timestamp":1645666538441,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"outputId":"49c85f1d-ee76-4e53-8499-4323916346f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(False, False)"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"q15IDpMUYyel","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645666538442,"user_tz":-540,"elapsed":11,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"outputId":"fe27c194-d635-4644-b2f7-67df52b49e5c"},"source":["#Generator's output is between -1 and 1.\n","#So we need min-max scaling.\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","scaler.fit(time_series)\n","X_train = scaler.transform(X_train)\n","X_test = scaler.transform(X_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n","  \"X does not have valid feature names, but\"\n","/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n","  \"X does not have valid feature names, but\"\n"]}]},{"cell_type":"code","source":["max(X_train), min(X_train), max(X_test), min(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdGUbRqocrfo","executionInfo":{"status":"ok","timestamp":1645666538442,"user_tz":-540,"elapsed":10,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"outputId":"5e8cdd83-66cb-4429-91ee-521777a4957a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0.28884987]), array([-1.]), array([1.]), array([-1.]))"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"YuN0SmzCZ4gx"},"source":["# def rolling_window_sequences(X, index, window_size, target_size, step_size, target_column,\n","#                              drop=None, drop_windows=False):\n","def rolling_window_sequences(X, y, window_size, step_size):\n","    \"\"\"Create rolling window sequences out of time series data.\n","    The function creates an array of input sequences and an array of target sequences by rolling\n","    over the input sequence with a specified window.\n","    Optionally, certain values can be dropped from the sequences.\n","    Args:\n","        X (ndarray):\n","            N-dimensional sequence to iterate over.\n","        index (ndarray):\n","            Array containing the index values of X.\n","        window_size (int):\n","            Length of the input sequences.\n","        target_size (int):\n","            Length of the target sequences.\n","        step_size (int):\n","            Indicating the number of steps to move the window forward each round.\n","        target_column (int):\n","            Indicating which column of X is the target.\n","        drop (ndarray or None or str or float or bool):\n","            Optional. Array of boolean values indicating which values of X are invalid, or value\n","            indicating which value should be dropped. If not given, `None` is used.\n","        drop_windows (bool):\n","            Optional. Indicates whether the dropping functionality should be enabled. If not\n","            given, `False` is used.\n","    Returns:\n","        ndarray, ndarray, ndarray, ndarray:\n","            * input sequences.\n","            * target sequences.\n","            * first index value of each input sequence.\n","            * first index value of each target sequence.\n","    \"\"\"\n","    out_X = list()\n","    out_y = list()\n","    # X_index = list()\n","    # y_index = list()\n","    # target = X[:, target_column]\n","    target = y.values\n","\n","    # if drop_windows:\n","    #     if hasattr(drop, '__len__') and (not isinstance(drop, str)):\n","    #         if len(drop) != len(X):\n","    #             raise Exception('Arrays `drop` and `X` must be of the same length.')\n","    #     else:\n","    #         if isinstance(drop, float) and np.isnan(drop):\n","    #             drop = np.isnan(X)\n","    #         else:\n","    #             drop = X == drop\n","\n","    start = 0\n","    max_start = len(X) - window_size + 1\n","    while start < max_start:\n","        end = start + window_size\n","\n","        # if drop_windows:\n","        #     drop_window = drop[start:end + target_size]\n","        #     to_drop = np.where(drop_window)[0]\n","        #     if to_drop.size:\n","        #         start += to_drop[-1] + 1\n","        #         continue\n","\n","        out_X.append(X[start:end])\n","        # out_y.append(target[end:end + target_size])\n","        if True in target[start:end]:\n","            out_y.append(True)\n","        else:\n","            out_y.append(False)\n","        # X_index.append(index[start])\n","        # y_index.append(index[end])\n","        start = start + step_size\n","\n","    # return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)\n","    return np.asarray(out_X), np.asarray(out_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3oekRlSZ6Wn"},"source":["X_train_ts, y_train_ts = rolling_window_sequences(X_train, y_train, window_size=100, step_size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test_ts, y_test_ts = rolling_window_sequences(X_test, y_test, window_size=100, step_size=1)"],"metadata":{"id":"KarF6JVHsOhJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anomaly_num = sum(y_test_ts)"],"metadata":{"id":"M39i5KZ1YxUj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Prepare4"],"metadata":{"id":"7ne7d58Ti-k1"}},{"cell_type":"code","metadata":{"id":"PhYaWBwpZ-Az","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645666538828,"user_tz":-540,"elapsed":10,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"outputId":"1403d3ab-a31b-4f3c-a463-5f07ece634e3"},"source":["print(\"Training data input shape: {}\".format(X_train_ts.shape))\n","print(\"Training y shape: {}\".format(y_train_ts.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data input shape: (506, 100, 1)\n","Training y shape: (506,)\n"]}]},{"cell_type":"markdown","source":["##Train all"],"metadata":{"id":"p6J5SAkuamnv"}},{"cell_type":"markdown","metadata":{"id":"EjqfKEg9aApL"},"source":["###Train TadSAGAN\n","\n","- GPU check for TadGAN \n","- Load Tensorflow, Keras, Layers .."]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6KQmKjO8DD6","executionInfo":{"status":"ok","timestamp":1644228798182,"user_tz":-540,"elapsed":4,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}},"outputId":"d37c67ea-cf9e-420e-c4b0-e23fb664b546"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Feb  7 10:13:19 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"7ys30Y68EtM8"},"source":["from sklearn.metrics import classification_report\n","from scipy.spatial.distance import euclidean\n","from fastdtw import fastdtw\n","\n","import tensorflow as tf\n","import keras\n","#import similaritymeasures as sm\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","\n","from tensorflow.keras.layers import Bidirectional, LSTM, GRU, Flatten, Dense, Reshape, UpSampling1D, TimeDistributed, Conv1DTranspose\n","from tensorflow.keras.layers import Activation, Conv1D, LeakyReLU, Dropout, Add, Layer, Concatenate\n","from tensorflow.keras.optimizers import Adam\n","\n","from functools import partial\n","from scipy import integrate, stats\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SelfAttention(tf.keras.models.Model):\n","    '''\n","    Multi-head ではない単純な Self Attention\n","    '''\n","\n","    def __init__(self, depth: int, *args, **kwargs):\n","        '''\n","        コンストラクタ。\n","        :param depth: 隠れ層及び出力の次元\n","        '''\n","        super().__init__(*args, **kwargs)\n","        self.depth = depth\n","\n","        self.q_dense_layer = tf.keras.layers.Dense(depth, use_bias=False, name='q_dense_layer')\n","        self.k_dense_layer = tf.keras.layers.Dense(depth, use_bias=False, name='k_dense_layer')\n","        self.v_dense_layer = tf.keras.layers.Dense(depth, use_bias=False, name='v_dense_layer')\n","        self.output_dense_layer = tf.keras.layers.Dense(depth, use_bias=False, name='output_dense_layer')\n","\n","    def call(self, input: tf.Tensor) -> tf.Tensor:\n","        '''\n","        モデルの実行。\n","        :param input: query のテンソル\n","        :param memory: query に情報を与える memory のテンソル\n","        '''\n","        q = self.q_dense_layer(input)  # [batch_size, q_length, depth]\n","        k = self.k_dense_layer(input)  # [batch_size, m_length, depth]\n","        v = self.v_dense_layer(input)\n","\n","        # ここで q と k の内積を取ることで、query と key の関連度のようなものを計算。\n","        logit = tf.matmul(q, k, transpose_b=True)  # [batch_size, q_length, k_length]\n","\n","        # softmax を取ることで正規化\n","        attention_weight = tf.nn.softmax(logit, name='attention_weight')\n","\n","        # 重みに従って value から情報を引く\n","        attention_output = tf.matmul(attention_weight, v)  # [batch_size, q_length, depth]\n","        return self.output_dense_layer(attention_output)"],"metadata":{"id":"0Jdo_VU_ypRK"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fDOTybi0OvWV"},"source":["#データ取ったのは、GとEのLeakyReLUは存在しないバージョン\n","def build_encoder_layer(input_shape, encoder_reshape_shape):\n","    x = Input(shape=input_shape)\n","    model = tf.keras.models.Sequential([\n","        Conv1D(64,5), #元は64,5\n","        # LSTM(units=100, return_sequences=True), \n","        SelfAttention(depth=64),\n","        Dense(64, activation=LeakyReLU(alpha=0.2)),\n","        Flatten(),\n","        Dense(20, activation=LeakyReLU(alpha=0.2)),\n","        Reshape(target_shape=encoder_reshape_shape)])\n","    return Model(x, model(x))\n","\n","def build_generator_layer(input_shape, generator_reshape_shape):\n","    x = Input(shape=input_shape)\n","    model = tf.keras.models.Sequential([\n","        Flatten(),\n","        Dense(50, activation=LeakyReLU(alpha=0.2)), #50\n","        Reshape(target_shape=(50,1)),  # (50, 1)\n","        LSTM(units=64, return_sequences=True),\n","        Dropout(rate=0.2),\n","        UpSampling1D(size=2),\n","        LSTM(units=64, return_sequences=True),\n","        Dropout(rate=0.2),\n","        SelfAttention(depth=64),\n","        Dense(32, activation=LeakyReLU(alpha=0.2)),\n","        TimeDistributed(Dense(1, activation=LeakyReLU(alpha=0.2))),\n","        Activation(activation='tanh')\n","        ])  # (None, 100, 1)\n","\n","    return Model(x, model(x))\n","\n","def build_critic_layer(x_input_shape, z_input_shape):\n","\n","    #C(x)\n","    x_in = Input(shape=x_input_shape)\n","    x = Conv1D(filters=32, kernel_size=5)(x_in) #64\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Dropout(rate=0.5)(x)\n","    x = Flatten()(x)\n","    \n","    #C(z)\n","    z_in = Input(shape=z_input_shape)\n","    z = Flatten()(z_in)\n","    z = Dense(units=32)(z) #100\n","    z = LeakyReLU(alpha=0.2)(z)\n","    z = Dropout(rate=0.2)(z)\n","\n","    x_z = Concatenate(axis=1)([x, z])\n","\n","    x_z = Dense(units=32)(x_z) #128\n","    fd = LeakyReLU(alpha=0.2)(x_z)\n","    x_z = Dense(units=1)(fd)\n","    output = Activation(\"sigmoid\")(x_z)\n","\n","    return Model([x_in, z_in],[fd, output])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8ZcWpI7OvWW"},"source":["# Layer Parameters\n","latent_dim = 20 #20\n","shape = (100, 1)\n","\n","encoder_input_shape = (100, 1)\n","generator_input_shape = (20, 1) #\n","critic_x_input_shape = (100, 1)\n","critic_z_input_shape = (20,1) #\n","encoder_reshape_shape = (20, 1) #\n","generator_reshape_shape = (50, 1)\n","learning_rate = 0.0005\n","\n","encoder = build_encoder_layer(input_shape=encoder_input_shape, encoder_reshape_shape=encoder_reshape_shape)\n","generator = build_generator_layer(input_shape=generator_input_shape, generator_reshape_shape=generator_reshape_shape)\n","critic = build_critic_layer(x_input_shape=critic_x_input_shape, z_input_shape=critic_z_input_shape)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tf.compat.v1.disable_eager_execution()"],"metadata":{"id":"ublOlCUtGNCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2EPMhkZLOvWW"},"source":["z = Input(shape=(latent_dim, 1))\n","x = Input(shape=shape)\n","x_ = generator(z)\n","z_ = encoder(x)\n","fake_fd, fake_x_z = critic([x_, z]) #Generator input & outputを入力 <- criticの学習ではこっちに騙されないように\n","valid_fd, valid_x_z = critic([x, z_]) #Encoder input & outputを入力 <- こっちを本物だと思うように学習\n","\n","critic_model = Model(inputs=[x, z], outputs=[fake_x_z, valid_x_z, fake_fd, valid_fd])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"43QYfgRCOvWX"},"source":["z_gen = Input(shape=(latent_dim, 1))\n","x_gen_ = generator(z_gen)\n","x_enc = Input(shape=shape)\n","z_enc_ = encoder(x_enc)\n","x_gen_rec = generator(z_enc_)\n","fake_enc_gen_fd, fake_enc_gen_x_z = critic([x_gen_, z_gen]) #Generator input & outputを入力 <- Generatorの学習ではこっちを本物のように\n","valid_enc_gen_fd, valid_enc_gen_x_z = critic([x_enc, z_enc_]) #Encoder input & outputを入力 <- Encoderの学習ではこっちを偽物のように学習\n","\n","encoder_generator_model = Model([x_enc, z_gen], [fake_enc_gen_x_z, valid_enc_gen_x_z, x_gen_rec, fake_enc_gen_fd, valid_enc_gen_fd])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pp7JdciKOvWX"},"source":["@tf.function\n","def critic_train_on_batch(x, z, valid, fake):\n","    with tf.GradientTape() as tape:\n","        \n","        fake_x_z, valid_x_z, _, _ = critic_model(inputs=[x, z], training=True) \n","\n","        # if np.random.random(1) < 0.8:\n","        #     loss = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(valid, valid_x_z))\n","        #     loss += tf.reduce_mean(tf.keras.metrics.binary_crossentropy(fake, fake_x_z))\n","        # else:\n","        #     loss = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(fake, valid_x_z))\n","        #     loss += tf.reduce_mean(tf.keras.metrics.binary_crossentropy(valid, fake_x_z))\n","        loss = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(valid, valid_x_z))\n","        loss += tf.reduce_mean(tf.keras.metrics.binary_crossentropy(fake, fake_x_z))\n","        \n","    gradients = tape.gradient(loss, critic_model.trainable_weights)\n","    optimizer.apply_gradients(zip(gradients, critic_model.trainable_weights))\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Rkpv6veOvWY"},"source":["@tf.function\n","def enc_gen_train_on_batch(x, z, valid):\n","    with tf.GradientTape() as tape:\n","        \n","        fake_enc_gen_x_z, valid_enc_gen_x_z, x_gen_rec, _, _ = encoder_generator_model(inputs=[x, z], training=True)\n","        \n","        x = tf.squeeze(x)\n","        x_gen_rec = tf.squeeze(x_gen_rec)\n","        \n","        loss = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(valid, fake_enc_gen_x_z))\n","        loss += tf.reduce_mean(tf.keras.metrics.binary_crossentropy(fake, valid_enc_gen_x_z))\n","        loss += tf.keras.losses.MSE(x, x_gen_rec)*10 #10\n","        loss = tf.reduce_mean(loss)\n","        \n","    gradients = tape.gradient(loss, encoder_generator_model.trainable_weights)\n","    optimizer.apply_gradients(zip(gradients, encoder_generator_model.trainable_weights))\n","    \n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ucvXt9XuOvWZ"},"source":["# Train parameters\n","batch_size = 64\n","n_critics = 1\n","epochs = 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QPb7zt-gc80n"},"source":["c_losses = []\n","g_losses = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQ7EjnR4OvWZ","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1fbtkisLzryh5W5TVP33Nn6qxiOwrlfRF"},"outputId":"fc8659ef-07be-4d61-a56a-7df9f951b2b6","executionInfo":{"status":"ok","timestamp":1644228926995,"user_tz":-540,"elapsed":120541,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GglOWMNcXjVJfCmwDvLe9-rK6XZwwQ4dtaGzxKPCA=s64","userId":"13956305302802793841"}}},"source":["# Train \n","X_train_ts_ = np.copy(X_train_ts)\n","\n","fake = np.zeros((batch_size, 1), dtype=np.float32)\n","valid = np.ones((batch_size, 1), dtype=np.float32)\n","\n","tsg_best_cr = 0\n","tsg_best_f1 = 0.0\n","\n","for epoch in range(1, epochs+1):\n","    \n","    np.random.shuffle(X_train_ts_)\n","    \n","    epoch_g_loss = []\n","    epoch_c_loss = []\n","    \n","    minibatches_size = batch_size * n_critics\n","    num_minibatches = int(X_train_ts_.shape[0] // minibatches_size)\n","    \n","    for i in range(num_minibatches):\n","        minibatch = X_train_ts_[i * minibatches_size: (i + 1) * minibatches_size]\n","        \n","        for j in range(n_critics):\n","            #train critic\n","            generator.trainable = False\n","            encoder.trainable = False\n","            critic.trainable = True \n","            x = minibatch[j * batch_size: (j + 1) * batch_size]\n","            z = np.random.normal(size=(batch_size, latent_dim, 1))\n","            epoch_c_loss.append(critic_train_on_batch(x, z, valid, fake))\n","            \n","            # train encoder, generator  \n","            critic.trainable = False\n","            generator.trainable = True\n","            encoder.trainable = True   \n","            for _ in range(10): # 10 → 5\n","                epoch_g_loss.append(enc_gen_train_on_batch(x, z, valid))\n","\n","    plt.plot(X_train_ts[0])\n","    plt.plot(generator(encoder(X_train_ts[0].reshape(1,100,1)))[0])\n","    plt.show() \n","        \n","    c_loss = np.mean(np.array(epoch_c_loss), axis=0)\n","    g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n","    print('Epoch: {}/{}, [C loss: {}] [G loss: {}]'.format(epoch, epochs, c_loss, g_loss))\n","\n","    if epoch%1 == 0:\n","        # Get test MSE loss.\n","        test_enc_out = encoder.predict(X_test_ts)\n","        X_test_ts_pred = generator(test_enc_out)\n","\n","        #reconstruct loss\n","        mse_losses = np.mean(np.mean(np.square(X_test_ts_pred - X_test_ts), axis=-1),axis=-1)\n","        mae_losses = np.mean(np.mean(np.abs(X_test_ts_pred - X_test_ts), axis=-1),axis=-1)\n","        # dtw_losses = []\n","        # for pred_ts, test_ts in zip(X_test_ts_pred, X_test_ts):\n","        #     dtw_loss, _ = fastdtw(pred_ts, test_ts, dist=euclidean) #dtw\n","        #     dtw_losses.append(dtw_loss)\n","        mse_losses = stats.zscore(mse_losses)\n","        mae_losses = stats.zscore(mae_losses)\n","        # dtw_losses = stats.zscore(dtw_losses)\n","        mse_losses = np.clip(mse_losses, a_min=0, a_max=None) + 1\n","        mae_losses = np.clip(mae_losses, a_min=0, a_max=None) + 1\n","        # dtw_losses = np.clip(dtw_losses, a_min=0, a_max=None) + 1\n","\n","        #critic loss\n","        test_fake_fd_val, _ = critic([X_test_ts_pred, test_enc_out])\n","        test_valid_fd_val, _ = critic([X_test_ts, test_enc_out])\n","        test_critic_scores = np.mean(np.square(test_fake_fd_val - test_valid_fd_val), axis=-1)\n","        test_critic_scores = stats.zscore(test_critic_scores)\n","        test_critic_scores = np.clip(test_critic_scores, a_min=0, a_max=None) + 1\n","\n","        #combination loss\n","        # test_anomaly_scores = test_critic_loss * np.array(rec_loss)\n","        # test_anomaly_scores = test_critic_loss + np.array(rec_loss)\n","\n","        #anomaly detection\n","        ##rec_loss\n","        ###mse\n","        test_pred = np.zeros(len(y_test_ts))\n","        anomalies_idx = np.argsort(mse_losses)[::-1][0:anomaly_num]\n","        test_pred[anomalies_idx] = 1\n","        # print(\"mse\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n","        if f1_score(y_test_ts, test_pred) > tsg_best_f1:\n","            tsg_best_f1 = f1_score(y_test_ts, test_pred)\n","            tsg_best_cr = classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4)\n","        ###mae\n","        test_pred = np.zeros(len(y_test_ts))\n","        anomalies_idx = np.argsort(mae_losses)[::-1][0:anomaly_num]\n","        test_pred[anomalies_idx] = 1\n","        # print(\"mae\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n","        if f1_score(y_test_ts, test_pred) > tsg_best_f1:\n","            tsg_best_f1 = f1_score(y_test_ts, test_pred)\n","            tsg_best_cr = classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4)\n","        ###dtw\n","        # test_pred = np.zeros(len(y_test_ts))\n","        # anomalies_idx = np.argsort(dtw_losses)[::-1][0:anomaly_num]\n","        # test_pred[anomalies_idx] = 1\n","        # print(\"dtw\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n","        ##critic_loss\n","        test_pred = np.zeros(len(y_test_ts))\n","        anomalies_idx = np.argsort(test_critic_scores)[::-1][0:anomaly_num]\n","        test_pred[anomalies_idx] = 1\n","        # print(\"critic\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n","        if f1_score(y_test_ts, test_pred) > tsg_best_f1:\n","            tsg_best_f1 = f1_score(y_test_ts, test_pred)\n","            tsg_best_cr = classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4)\n","        ##combination_loss\n","        ###mse\n","        test_pred = np.zeros(len(y_test_ts))\n","        test_anomaly_scores = test_critic_scores * np.array(mse_losses)\n","        anomalies_idx = np.argsort(test_anomaly_scores)[::-1][0:anomaly_num]\n","        test_pred[anomalies_idx] = 1\n","        # print(\"mse x critic\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n","        if f1_score(y_test_ts, test_pred) > tsg_best_f1:\n","            tsg_best_f1 = f1_score(y_test_ts, test_pred)\n","            tsg_best_cr = classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4)\n","        test_pred = np.zeros(len(y_test_ts))\n","        test_anomaly_scores = test_critic_scores + np.array(mse_losses)\n","        anomalies_idx = np.argsort(test_anomaly_scores)[::-1][0:anomaly_num]\n","        test_pred[anomalies_idx] = 1\n","        # print(\"mse + critic\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n","        if f1_score(y_test_ts, test_pred) > tsg_best_f1:\n","            tsg_best_f1 = f1_score(y_test_ts, test_pred)\n","            tsg_best_cr = classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4)\n","        ###mae\n","        test_pred = np.zeros(len(y_test_ts))\n","        test_anomaly_scores = test_critic_scores * np.array(mae_losses)\n","        anomalies_idx = np.argsort(test_anomaly_scores)[::-1][0:anomaly_num]\n","        test_pred[anomalies_idx] = 1\n","        # print(\"mae x critic\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n","        if f1_score(y_test_ts, test_pred) > tsg_best_f1:\n","            tsg_best_f1 = f1_score(y_test_ts, test_pred)\n","            tsg_best_cr = classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4)\n","        test_pred = np.zeros(len(y_test_ts))\n","        test_anomaly_scores = test_critic_scores + np.array(mae_losses)\n","        anomalies_idx = np.argsort(test_anomaly_scores)[::-1][0:anomaly_num]\n","        test_pred[anomalies_idx] = 1\n","        # print(\"mae + critic\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n","        if f1_score(y_test_ts, test_pred) > tsg_best_f1:\n","            tsg_best_f1 = f1_score(y_test_ts, test_pred)\n","            tsg_best_cr = classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4)\n","        ###dtw\n","        # test_pred = np.zeros(len(y_test_ts))\n","        # test_anomaly_scores = test_critic_scores * np.array(dtw_losses)\n","        # anomalies_idx = np.argsort(test_anomaly_scores)[::-1][0:anomaly_num]\n","        # test_pred[anomalies_idx] = 1\n","        # print(\"dtw x critic\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n","        # test_pred = np.zeros(len(y_test_ts))\n","        # test_anomaly_scores = test_critic_scores + np.array(dtw_losses)\n","        # anomalies_idx = np.argsort(test_anomaly_scores)[::-1][0:anomaly_num]\n","        # test_pred[anomalies_idx] = 1\n","        # print(\"dtw + critic\")\n","        # print(classification_report(y_test_ts, test_pred, target_names=[\"正常\",\"異常\"], digits=4))\n"," \n","    c_losses.append(c_loss)\n","    g_losses.append(g_loss)\n","print(tsg_best_f1)\n","print(tsg_best_cr)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["##Result"],"metadata":{"id":"HP00-9BKjF4U"}},{"cell_type":"code","source":["print(\"tsg\")\n","print(tsg_best_cr)"],"metadata":{"id":"ltPHYNFFjHgY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643519206315,"user_tz":-540,"elapsed":12,"user":{"displayName":"Tatsuki Kawamoto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjyLX1acDACXKF4ip1DM7kf8ajMbSemmpXQUuSO9Q=s64","userId":"17762593086322842396"}},"outputId":"73f30f9b-c1d5-438c-bcde-c13cc7609176"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ae\n","              precision    recall  f1-score   support\n","\n","          正常     0.9417    0.9417    0.9417     12555\n","          異常     0.1202    0.1202    0.1202       832\n","\n","    accuracy                         0.8906     13387\n","   macro avg     0.5309    0.5309    0.5309     13387\n","weighted avg     0.8906    0.8906    0.8906     13387\n","\n","tg\n","              precision    recall  f1-score   support\n","\n","          正常     0.9451    0.9451    0.9451     12555\n","          異常     0.1719    0.1719    0.1719       832\n","\n","    accuracy                         0.8971     13387\n","   macro avg     0.5585    0.5585    0.5585     13387\n","weighted avg     0.8971    0.8971    0.8971     13387\n","\n","tsg\n","              precision    recall  f1-score   support\n","\n","          正常     0.9505    0.9505    0.9505     12555\n","          異常     0.2524    0.2524    0.2524       832\n","\n","    accuracy                         0.9071     13387\n","   macro avg     0.6014    0.6014    0.6014     13387\n","weighted avg     0.9071    0.9071    0.9071     13387\n","\n"]}]}]}